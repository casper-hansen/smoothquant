{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from smoothquant.smooth import smooth_lm\n",
    "from smoothquant.utils import Perplexity\n",
    "from smoothquant.llama import Int8LlamaForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from smoothquant.calibration import get_act_scales, get_static_llama_decoder_layer_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TheBloke/Llama-2-7b-chat-fp16'\n",
    "# model_name = 'PY007/TinyLlama-1.1B-Chat-v0.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f029c74b96940e9b94ba4ed3b794210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d5e5f481f94f2ca8ec06a739349b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity: - :   0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 perplexity: 7.627832972676341\n"
     ]
    }
   ],
   "source": [
    "ppl = Perplexity(model_fp16, tokenizer)\n",
    "out = ppl.calculate_perplexity()\n",
    "print(f'FP16 perplexity: {out[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2db807a2ce433981588044c2178ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "100%|██████████| 512/512 [01:01<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activation scales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/512 [00:00<?, ?it/s]Repo card metadata block was not found. Setting CardData to empty.\n",
      "Mean input scale: 20.86: 100%|██████████| 512/512 [01:02<00:00,  8.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# smooth layers\n",
    "act_scales = get_act_scales(model, tokenizer, 'mit-han-lab/pile-val-backup', 512, 512)\n",
    "smooth_lm(model, act_scales, 0.5)\n",
    "\n",
    "# get model scales\n",
    "decoder_layer_scales, raw_scales = get_static_llama_decoder_layer_scales(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    'mit-han-lab/pile-val-backup',\n",
    "    num_samples=512,\n",
    "    seq_len=512\n",
    ")\n",
    "\n",
    "with torch.device(\"cuda\"):\n",
    "    model_smoothquant_w8a8 = Int8LlamaForCausalLM.from_float(model, decoder_layer_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66eb8ee97f6c4e3cbfb7f303a69e4851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity: - :   0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/smoothquant/examples/smoothquant_llama_demo.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/smoothquant/examples/smoothquant_llama_demo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ppl \u001b[39m=\u001b[39m Perplexity(model_smoothquant_w8a8, tokenizer)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/smoothquant/examples/smoothquant_llama_demo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out \u001b[39m=\u001b[39m ppl\u001b[39m.\u001b[39;49mcalculate_perplexity()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brunpod/workspace/smoothquant/examples/smoothquant_llama_demo.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSmoothQuant W8A8 perplexity: \u001b[39m\u001b[39m{\u001b[39;00mout[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/workspace/smoothquant/smoothquant/utils.py:73\u001b[0m, in \u001b[0;36mPerplexity.calculate_perplexity\u001b[0;34m(self, n_ctx, n_batch, tokens)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tokens[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_ctx), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerplexity: - \u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m progress:\n\u001b[1;32m     72\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m progress:\n\u001b[0;32m---> 73\u001b[0m         nll, count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_batch(i, n_ctx, n_batch, tokens, nll, count)\n\u001b[1;32m     74\u001b[0m         curr_ppl \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(nll \u001b[39m/\u001b[39m count)\n\u001b[1;32m     75\u001b[0m         all_perplexity\u001b[39m.\u001b[39mappend(curr_ppl)\n",
      "File \u001b[0;32m/workspace/smoothquant/smoothquant/utils.py:100\u001b[0m, in \u001b[0;36mPerplexity._process_batch\u001b[0;34m(self, i, n_ctx, n_batch, tokens, nll, count)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mmin\u001b[39m(\u001b[39m512\u001b[39m, n_ctx \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m), n_ctx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     99\u001b[0m     tok_logits \u001b[39m=\u001b[39m logits[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][j]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m--> 100\u001b[0m     prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(tok_logits)[tokens[\u001b[39m0\u001b[39m][start \u001b[39m+\u001b[39m j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]]\n\u001b[1;32m    101\u001b[0m     nll \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog(prob, where\u001b[39m=\u001b[39mprob\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    102\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/workspace/smoothquant/smoothquant/utils.py:58\u001b[0m, in \u001b[0;36mPerplexity.softmax\u001b[0;34m(logits)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(logits):\n\u001b[0;32m---> 58\u001b[0m     e_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(logits \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39;49mmax(logits))\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m e_x \u001b[39m/\u001b[39m e_x\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:2820\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2703\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[1;32m   2704\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2705\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2706\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2707\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2818\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[1;32m   2819\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2820\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   2821\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppl = Perplexity(model_smoothquant_w8a8, tokenizer)\n",
    "out = ppl.calculate_perplexity()\n",
    "print(f'SmoothQuant W8A8 perplexity: {out[-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
