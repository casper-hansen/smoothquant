{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from smoothquant.smooth import smooth_lm\n",
    "from smoothquant.calibration import get_act_scales\n",
    "from smoothquant.utils import quantize_model, Perplexity\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'PY007/TinyLlama-1.1B-Chat-v0.2'\n",
    "\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f71ab8a33644ddca64cdcf6ba023c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity: - :   0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 perplexity: 13.785424512084473\n"
     ]
    }
   ],
   "source": [
    "ppl = Perplexity(model_fp16, tokenizer)\n",
    "tokens = tokenizer(ppl._text, truncation=False, return_tensors='pt').input_ids.to(ppl._model.device)\n",
    "out = ppl.calculate_perplexity(tokens=tokens)\n",
    "print(f'FP16 perplexity: {out[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive W8A8 Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0979916ac640868c5e99fb93e7583e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity: - :   0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive W8A8 perplexity: 40.489475107108944\n"
     ]
    }
   ],
   "source": [
    "model_w8a8 = quantize_model(model_fp16)\n",
    "ppl = Perplexity(model_fp16, tokenizer)\n",
    "out = ppl.calculate_perplexity(tokens=tokens)\n",
    "print(f'Naive W8A8 perplexity: {out[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "100%|██████████| 512/512 [00:31<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smooth llama decoder: model.layers.0\n",
      "smooth llama decoder: model.layers.1\n",
      "smooth llama decoder: model.layers.2\n",
      "smooth llama decoder: model.layers.3\n",
      "smooth llama decoder: model.layers.4\n",
      "smooth llama decoder: model.layers.5\n",
      "smooth llama decoder: model.layers.6\n",
      "smooth llama decoder: model.layers.7\n",
      "smooth llama decoder: model.layers.8\n",
      "smooth llama decoder: model.layers.9\n",
      "smooth llama decoder: model.layers.10\n",
      "smooth llama decoder: model.layers.11\n",
      "smooth llama decoder: model.layers.12\n",
      "smooth llama decoder: model.layers.13\n",
      "smooth llama decoder: model.layers.14\n",
      "smooth llama decoder: model.layers.15\n",
      "smooth llama decoder: model.layers.16\n",
      "smooth llama decoder: model.layers.17\n",
      "smooth llama decoder: model.layers.18\n",
      "smooth llama decoder: model.layers.19\n",
      "smooth llama decoder: model.layers.20\n",
      "smooth llama decoder: model.layers.21\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "act_scales = get_act_scales(\n",
    "    model, tokenizer, 'mit-han-lab/pile-val-backup', 512, 512\n",
    ")\n",
    "\n",
    "smooth_lm(model, act_scales, 0.5)\n",
    "model_smoothquant_w8a8 = quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b391c1537f754bbcad55a342dca0a7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perplexity: - :   0%|          | 0/655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmoothQuant W8A8 perplexity: 36.41558657884512\n"
     ]
    }
   ],
   "source": [
    "ppl = Perplexity(model_smoothquant_w8a8, tokenizer)\n",
    "out = ppl.calculate_perplexity(tokens=tokens)\n",
    "print(f'SmoothQuant W8A8 perplexity: {out[-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
