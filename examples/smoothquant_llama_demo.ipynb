{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from smoothquant.smooth import smooth_lm\n",
    "from smoothquant.utils import Perplexity\n",
    "from smoothquant.llama import Int8LlamaForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from smoothquant.calibration import get_act_scales, get_static_llama_decoder_layer_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TheBloke/Llama-2-7b-chat-fp16'\n",
    "# model_name = 'PY007/TinyLlama-1.1B-Chat-v0.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = Perplexity(model_fp16, tokenizer)\n",
    "out = ppl.calculate_perplexity()\n",
    "print(f'FP16 perplexity: {out[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# smooth layers\n",
    "act_scales = get_act_scales(model, tokenizer, 'mit-han-lab/pile-val-backup', 512, 512)\n",
    "smooth_lm(model, act_scales, 0.5)\n",
    "\n",
    "# get model scales\n",
    "decoder_layer_scales, raw_scales = get_static_llama_decoder_layer_scales(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    'mit-han-lab/pile-val-backup',\n",
    "    num_samples=512,\n",
    "    seq_len=512\n",
    ")\n",
    "\n",
    "model_smoothquant_w8a8 = Int8LlamaForCausalLM.from_float(model, decoder_layer_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = Perplexity(model_smoothquant_w8a8, tokenizer)\n",
    "out = ppl.calculate_perplexity()\n",
    "print(f'SmoothQuant W8A8 perplexity: {out[-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
